{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 1.52 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\rache\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Guide to download emoji in\n",
    "\n",
    "# import demoji\n",
    "# demoji.download_codes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import demoji\n",
    "import sys\n",
    "import re as re\n",
    "import nltk\n",
    "import string, unicodedata\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12506 entries, 0 to 12505\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed:_0        12506 non-null  int64  \n",
      " 1   Platform          12506 non-null  object \n",
      " 2   Brand             12506 non-null  object \n",
      " 3   Category          12506 non-null  object \n",
      " 4   Product_Name      12506 non-null  object \n",
      " 5   Price             12506 non-null  object \n",
      " 6   Reviewer          12462 non-null  object \n",
      " 7   Review            12506 non-null  object \n",
      " 8   Product_Purchase  8593 non-null   object \n",
      " 9   Ratings           12462 non-null  float64\n",
      " 10  Date_Of_Review    12462 non-null  object \n",
      " 11  Response          12462 non-null  object \n",
      "dtypes: float64(1), int64(1), object(10)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed:_0</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Category</th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Reviewer</th>\n",
       "      <th>Review</th>\n",
       "      <th>Product_Purchase</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Date_Of_Review</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Lazada</td>\n",
       "      <td>Garnier</td>\n",
       "      <td>Makeup Removers</td>\n",
       "      <td>Micellar Cleansing Water All-in-1 Biphase (For...</td>\n",
       "      <td>$13.59</td>\n",
       "      <td>Jhudelin R.</td>\n",
       "      <td>recieved the items timely however when i open ...</td>\n",
       "      <td>Volume (ml):400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13-Feb-20</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lazada</td>\n",
       "      <td>Garnier</td>\n",
       "      <td>Makeup Removers</td>\n",
       "      <td>Micellar Cleansing Water All-in-1 Biphase (For...</td>\n",
       "      <td>$13.59</td>\n",
       "      <td>Jasmine G.</td>\n",
       "      <td>good buy at a bundle deal! one of my favourite...</td>\n",
       "      <td>Volume (ml):400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29-Dec-18</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lazada</td>\n",
       "      <td>Garnier</td>\n",
       "      <td>Makeup Removers</td>\n",
       "      <td>Micellar Cleansing Water All-in-1 Biphase (For...</td>\n",
       "      <td>$13.59</td>\n",
       "      <td>Lazada Customer</td>\n",
       "      <td>purchased 3 bottles during the festival. have ...</td>\n",
       "      <td>Volume (ml):400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19-Jul-19</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Lazada</td>\n",
       "      <td>Garnier</td>\n",
       "      <td>Makeup Removers</td>\n",
       "      <td>Micellar Cleansing Water All-in-1 Biphase (For...</td>\n",
       "      <td>$13.59</td>\n",
       "      <td>Lazada Customer</td>\n",
       "      <td>fast delivery. hassle free.</td>\n",
       "      <td>Volume (ml):400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11-Sep-19</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Lazada</td>\n",
       "      <td>Garnier</td>\n",
       "      <td>Makeup Removers</td>\n",
       "      <td>Micellar Cleansing Water All-in-1 Biphase (For...</td>\n",
       "      <td>$13.59</td>\n",
       "      <td>wendy A.</td>\n",
       "      <td>well received. delivery is consider fast too a...</td>\n",
       "      <td>Volume (ml):400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17-Dec-19</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed:_0 Platform    Brand         Category  \\\n",
       "0           0   Lazada  Garnier  Makeup Removers   \n",
       "1           1   Lazada  Garnier  Makeup Removers   \n",
       "2           2   Lazada  Garnier  Makeup Removers   \n",
       "3           3   Lazada  Garnier  Makeup Removers   \n",
       "4           4   Lazada  Garnier  Makeup Removers   \n",
       "\n",
       "                                        Product_Name    Price  \\\n",
       "0  Micellar Cleansing Water All-in-1 Biphase (For...  $13.59    \n",
       "1  Micellar Cleansing Water All-in-1 Biphase (For...  $13.59    \n",
       "2  Micellar Cleansing Water All-in-1 Biphase (For...  $13.59    \n",
       "3  Micellar Cleansing Water All-in-1 Biphase (For...  $13.59    \n",
       "4  Micellar Cleansing Water All-in-1 Biphase (For...  $13.59    \n",
       "\n",
       "           Reviewer                                             Review  \\\n",
       "0       Jhudelin R.  recieved the items timely however when i open ...   \n",
       "1        Jasmine G.  good buy at a bundle deal! one of my favourite...   \n",
       "2   Lazada Customer  purchased 3 bottles during the festival. have ...   \n",
       "3   Lazada Customer                        fast delivery. hassle free.   \n",
       "4          wendy A.  well received. delivery is consider fast too a...   \n",
       "\n",
       "  Product_Purchase  Ratings Date_Of_Review Response  \n",
       "0  Volume (ml):400      3.0      13-Feb-20       no  \n",
       "1  Volume (ml):400      5.0      29-Dec-18      yes  \n",
       "2  Volume (ml):400      5.0      19-Jul-19       no  \n",
       "3  Volume (ml):400      5.0      11-Sep-19       no  \n",
       "4  Volume (ml):400      5.0      17-Dec-19       no  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = \"all_data_raw.csv\"\n",
    "data = pd.read_csv(data_file)\n",
    "data.columns = data.columns.str.strip().str.replace(\" \",\"_\") # reason for doing this is cause cannot get columns with space into a list\n",
    "\n",
    "#lower case\n",
    "data['Review'] = [word.lower() for word in data['Review']]\n",
    "\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation and Initalise of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dataset is: 12506\n"
     ]
    }
   ],
   "source": [
    "# make the old csv data into a list\n",
    "id_list = data['Unnamed:_0'].tolist()\n",
    "platform_list = data['Platform'].tolist()\n",
    "brand_list = data['Brand'].tolist()\n",
    "category_list = data['Category'].tolist()\n",
    "product_name_list = data['Product_Name'].tolist()\n",
    "price_list = data['Price'].tolist()\n",
    "reviewer_list = data['Reviewer'].tolist()\n",
    "review_list = data['Review'].tolist()\n",
    "product_variation_list = data['Product_Purchase'].tolist()\n",
    "rating_list = data['Ratings'].tolist()\n",
    "date_review_list = data[\"Date_Of_Review\"].tolist()\n",
    "response_list = data[\"Response\"].tolist()\n",
    "\n",
    "# initialising the new columns\n",
    "id_csv=[]\n",
    "platform_csv=[]\n",
    "brand_csv = []\n",
    "category_csv = []\n",
    "product_name_csv = []\n",
    "prices_csv = []\n",
    "reviewer_csv =[]\n",
    "review_splitted_csv = []\n",
    "review_csv=[]\n",
    "product_variation_csv = []\n",
    "rating_csv = []\n",
    "date_review_csv = []\n",
    "response_csv = []\n",
    "emoji_csv =[]\n",
    "\n",
    "# this is the length of the csv old data\n",
    "print (\"The length of the dataset is:\", len(brand_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "def lemmatization(texts):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    doc = sp(texts)\n",
    "    \n",
    "    texts_out = ([token.lemma_ for token in doc])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the appos\n",
    "from word_dict import appos\n",
    "# print(appos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New stopWord list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "to_remove = ['not', 'no','but','if']\n",
    "new_stopwords = set(stopwords.words('english')).difference(to_remove)\n",
    "# print(new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Date Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brand_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3f9a541ae72f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnew_date\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrand_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdate_review\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate_review_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'brand_list' is not defined"
     ]
    }
   ],
   "source": [
    "new_date =[]\n",
    "for i in range(len(brand_list)):\n",
    "    \n",
    "    date_review = date_review_list[i]\n",
    "    \n",
    "    scrape_date = datetime(2020, 3, 23)\n",
    "    \n",
    "    if platform_list[i] == 'Lazada' and not pd.isna(date_review) :\n",
    "        if 'days ago' in date_review or 'day ago' in date_review :\n",
    "            day = date_review.replace('days ago', '').replace('day ago', '')\n",
    "            date_review = scrape_date - timedelta(days=int(day))\n",
    "            date_review = date_review.strftime(\"%d-%m-%y\")\n",
    "#             print(\"trans\",date_review)\n",
    "\n",
    "        if 'weeks ago' in date_review or 'week ago' in date_review:\n",
    "            day = date_review.replace('weeks ago', '').replace('week ago', '')\n",
    "            day = int(day)*7\n",
    "            date_review = scrape_date - timedelta(days=day)\n",
    "            date_review = date_review.strftime(\"%d-%m-%y\")\n",
    "#             print(\"trans\",date_review)\n",
    "\n",
    "        if 'hours ago' in date_review or 'hour ago' in date_review:\n",
    "            hours = date_review.replace('hours ago', '').replace('hour ago', '')\n",
    "            date_review = scrape_date - timedelta(hours=int(hours))\n",
    "            date_review = date_review.strftime(\"%d-%m-%y\")\n",
    "    \n",
    "#       print(\"trans\",date_review)\n",
    "        new_date.append(date_review)\n",
    "    else:\n",
    "#         print(platform_list[i])\n",
    "        new_date.append(date_review)\n",
    "        \n",
    "data[\"Date_Of_Review\"] = new_date\n",
    "\n",
    "new_date_review_list = data[\"Date_Of_Review\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21354 entries, 0 to 21353\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   ID                21354 non-null  int64         \n",
      " 1   Platform          21354 non-null  object        \n",
      " 2   Brand             21354 non-null  object        \n",
      " 3   Category          21354 non-null  object        \n",
      " 4   Product Name      21354 non-null  object        \n",
      " 5   Price             21354 non-null  object        \n",
      " 6   Reviewer          21310 non-null  object        \n",
      " 7   Review            21354 non-null  object        \n",
      " 8   Product Purchase  15363 non-null  object        \n",
      " 9   Ratings           21310 non-null  float64       \n",
      " 10  Date Of Review    21310 non-null  datetime64[ns]\n",
      " 11  Response          21310 non-null  object        \n",
      " 12  Review_splitted   21354 non-null  object        \n",
      " 13  Emoji             21354 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(11)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "for i in range(len(brand_list)):\n",
    "    # current_review is a string\n",
    "    current_review = review_list[i]\n",
    "    current_review = current_review.strip().replace(\"â€™\",\"'\")\n",
    "    \n",
    "    sentences = sent_tokenize(current_review)\n",
    "    \n",
    "    #print(processed_review)\n",
    "    for j in range(len(sentences)):\n",
    "        emoji_extract=\"\"\n",
    "        processed_review = sentences[j]\n",
    "        \n",
    "        # extract emoji from each sentence\n",
    "        emoji_dictionary = demoji.findall(processed_review)\n",
    "        if len(emoji_dictionary) == 0:\n",
    "            emoji_extract=\"\"\n",
    "        else:\n",
    "            temp_list = \"\"\n",
    "            for key in emoji_dictionary:\n",
    "                temp_list += key + \" \"\n",
    "            temp_list = temp_list[0:-1]\n",
    "            emoji_extract= temp_list\n",
    "        \n",
    "        # Negation handling\n",
    "        processed_review = processed_review.split()\n",
    "        processed_review =[appos[w] if w in appos else w for w in processed_review]\n",
    "        processed_review = \" \".join(processed_review) \n",
    "\n",
    "        processed_review = processed_review.strip()\n",
    "        \n",
    "        # Remove all the special characters\n",
    "        processed_review = re.sub(r'\\W', ' ', processed_review)\n",
    "        # Removing prefixed 'b'\n",
    "        processed_review = re.sub(r'^b\\s+', '', processed_review)\n",
    "        # remove all single characters contains a white space character\n",
    "        processed_review= re.sub(r'^[a-zA-Z]$', ' ', processed_review)\n",
    "        # Substituting multiple spaces with single space\n",
    "        processed_review = re.sub(r'\\s+', ' ', processed_review, flags=re.I)\n",
    "        processed = processed_review.strip()\n",
    "\n",
    "        \n",
    "        # final check if the processed_review is empty\n",
    "        final = []\n",
    "        if processed != \"\":\n",
    "            tokenized = word_tokenize(processed)\n",
    "            #removed additional letters eg \"gooooooodddddd\" to \"good\"\n",
    "            for w in tokenized:\n",
    "                processed_2 = reduce_lengthening(w)\n",
    "                processed_2 = spell.correction(processed_2)\n",
    "                final.append(processed_2)\n",
    "            final =' '.join(final)\n",
    "            \n",
    "            # append into a dictionary\n",
    "            id_csv.append(id_list[i])\n",
    "            platform_csv.append(platform_list[i])\n",
    "            brand_csv.append(brand_list[i])\n",
    "            category_csv.append(category_list[i])\n",
    "            product_name_csv.append(product_name_list[i])\n",
    "            prices_csv.append(price_list[i])\n",
    "            reviewer_csv.append(reviewer_list[i])\n",
    "            review_csv.append(review_list[i])\n",
    "            review_splitted_csv.append(final)\n",
    "            product_variation_csv.append(product_variation_list[i])\n",
    "            rating_csv.append(rating_list[i])\n",
    "            date_review_csv.append(new_date_review_list[i])\n",
    "            response_csv.append(response_list[i])\n",
    "            emoji_csv.append(emoji_extract)\n",
    "            \n",
    "#Store into a new data frame\n",
    "new_data = {'ID':id_csv,'Platform':platform_csv ,'Brand':brand_csv, 'Category': category_csv, 'Product_Name ': product_name_csv, 'Price':prices_csv ,'Reviewer':reviewer_csv,'Review':review_csv,'Product_Purchase':product_variation_csv,'Ratings':rating_csv,'Date_Of_Review':date_review_csv,'Response': response_csv,'Review_splitted':review_splitted_csv,'Emoji':emoji_csv}\n",
    "new_data = pd.DataFrame.from_dict(new_data)\n",
    "new_data['Date_Of_Review'] = pd.to_datetime(new_data['Date_Of_Review'])\n",
    "new_data.head()\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert the Emoji Ranking File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_sentiment_ranking_file = \"Emoji_Sentiment_Data_v1.0.csv\"\n",
    "emoji_sentiment_ranking = pd.read_csv(emoji_sentiment_ranking_file)\n",
    "emoji_sentiment_ranking.columns = emoji_sentiment_ranking.columns.str.strip().str.replace(\" \",\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_unicode = emoji_sentiment_ranking['Unicode_codepoint'].tolist()\n",
    "occurences = emoji_sentiment_ranking['Occurrences'].tolist()\n",
    "positive = emoji_sentiment_ranking['Positive'].tolist()\n",
    "neutral = emoji_sentiment_ranking['Neutral'].tolist()\n",
    "negative = emoji_sentiment_ranking['Negative'].tolist()\n",
    "\n",
    "def getEmojiSentiment(emoji_string):\n",
    "    positive_sentiment = 0\n",
    "    neutral_sentiment = 0\n",
    "    negative_sentiment = 0\n",
    "    emoji_splitted = emoji_string.split(\" \")\n",
    "    \n",
    "    for i in range(len(emoji_splitted)):\n",
    "        current_emoji = emoji_splitted[i]\n",
    "        current_emoji = current_emoji[0]\n",
    "        if len(current_emoji) == 1:\n",
    "            unicode = '0x{:X}'.format(ord(current_emoji)).lower()\n",
    "            \n",
    "            if unicode in emoji_unicode:\n",
    "                find_unicode_index = emoji_unicode.index(unicode)\n",
    "                \n",
    "                if int(positive[find_unicode_index]) >= int(neutral[find_unicode_index]) and int(positive[find_unicode_index]) >= int(negative[find_unicode_index]):\n",
    "                        positive_sentiment += 1\n",
    "                elif int(negative[find_unicode_index]) >= int(neutral[find_unicode_index]) and int(negative[find_unicode_index]) >= int(positive[find_unicode_index]):\n",
    "                        negative_sentiment += 1\n",
    "                elif int(neutral[find_unicode_index]) >= int(negative[find_unicode_index]) and int(neutral[find_unicode_index]) >= int(positive[find_unicode_index]):\n",
    "                        neutral_sentiment += 1\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "    if positive_sentiment > neutral_sentiment and positive_sentiment > negative_sentiment:\n",
    "        return (\"1\")\n",
    "    elif negative_sentiment > neutral_sentiment and negative_sentiment > positive_sentiment:\n",
    "        return (\"-1\")\n",
    "    else:\n",
    "        return (\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_polarity =[]\n",
    "emoji = new_data['Emoji'].tolist()\n",
    "\n",
    "for each_emoji in emoji:\n",
    "    current_review = each_emoji\n",
    "    print (current_review)\n",
    "    if current_review != \"\":\n",
    "        emoji_polarity.append(getEmojiSentiment(current_review))\n",
    "        print(getEmojiSentiment(current_review))\n",
    "    else:\n",
    "        emoji_polarity.append(\"\")\n",
    "    print (\"\\n\")\n",
    "\n",
    "new_data['Emoji_polarity'] = emoji_polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_review = new_data[\"Review_splitted\"]\n",
    "\n",
    "review_cleaned=[]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "sp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "for sentence in tokenized_review:\n",
    "    sentence = sentence.strip()\n",
    "    review = lemmatization(sentence)\n",
    "    review_stopremoved = [w for w in review if w not in new_stopwords]\n",
    "    review_cleaned.append(review_stopremoved)\n",
    "    \n",
    "new_data['tokenized_review'] = review_cleaned  \n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Positive , Negative , Increment , Decrement and Inverse Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "pos_lexicon = 'dict/positive-words.txt'\n",
    "neg_lexicon = 'dict/negative-words.txt'\n",
    "inc_words = 'dict/increment-words.txt'\n",
    "dec_words = 'dict/decrement-words.txt'\n",
    "inv_words = 'dict/inverse-words.txt'\n",
    "\n",
    "\n",
    "# Read the positive sentiment lexicon.\n",
    "pos_dict = {}\n",
    "f = open(pos_lexicon, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    pos_dict[line] = 1\n",
    "f.close()\n",
    "\n",
    "# Read the negative sentiment lexicon.\n",
    "neg_dict = {}\n",
    "f = open(neg_lexicon, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    neg_dict[line] = 1\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Read the increment words.\n",
    "inc_dict = {}\n",
    "f = open(inc_words, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    inc_dict[line] = 1\n",
    "f.close()\n",
    "\n",
    "# Read the decrement words.\n",
    "dec_dict = {}\n",
    "f = open(dec_words, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    dec_dict[line] = 1\n",
    "f.close()\n",
    "\n",
    "# Read the inverse words.\n",
    "inv_dict = {}\n",
    "f = open(inv_words, 'r', encoding = \"ISO-8859-1\")\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    inv_dict[line] = 1\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon-based Sentiment Polarity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "score_label=[]\n",
    "normalise_score_label=[]\n",
    "\n",
    "for sent in review_cleaned:\n",
    "    total_score = 0\n",
    "    print(sent)\n",
    "    for w in sent:\n",
    "        score = 0\n",
    "        if w in pos_dict:\n",
    "            score = 1\n",
    "        elif w in neg_dict:\n",
    "            score = -1\n",
    "        #check previous word\n",
    "        position = sent.index(w)\n",
    "        bigram_word = sent[(position-1):position+1]\n",
    "        print(\"bigram\",bigram_word)\n",
    "#         trigram_word = sent[(position-3):position]\n",
    "#         print(\"tigram\",trigram_word)\n",
    "        if len(bigram_word) > 0:\n",
    "            print(\"previous\",bigram_word[0])\n",
    "            if bigram_word[0]in inc_dict: \n",
    "                score *= 2.0\n",
    "            elif bigram_word[0]in dec_dict:\n",
    "                score /=2\n",
    "            elif bigram_word[0]in inv_dict:\n",
    "                score *= -1  \n",
    "        total_score+=score\n",
    "    if len(sent)> 0:\n",
    "        normalised_score  = total_score/(len(sent))\n",
    "    else:\n",
    "        normalised_score = 0\n",
    "    normalise_score_label.append(normalised_score)\n",
    "    \n",
    "    if normalised_score >0.1:\n",
    "        predicted_labels.append('1')\n",
    "        \n",
    "    elif normalised_score <-0.1:\n",
    "        predicted_labels.append('-1')\n",
    "        \n",
    "    else:\n",
    "        predicted_labels.append('0')\n",
    "        \n",
    "    score_label.append(int(total_score))\n",
    "    \n",
    "new_data[\"Polarity\"] = predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Polarity for Splitted Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_polarity =[]\n",
    "\n",
    "polarity = new_data['Polarity'].tolist()\n",
    "emoji_polarity = new_data['Emoji_polarity'].tolist()\n",
    "\n",
    "for i in range(len(polarity)):\n",
    "    sentiment_pol = int(polarity[i])\n",
    "    emoji_pol = int(emoji_polarity[i])\n",
    "    if emoji_pol == -1 and sentiment_pol > -1:\n",
    "        overall_polarity.append(\"-1\")\n",
    "    else:\n",
    "        overall_polarity.append(sentiment_pol)\n",
    "new_data[\"Overall_polarity\"] = overall_polarity\n",
    "new_data.to_csv('Data/all_data_clean_split.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
